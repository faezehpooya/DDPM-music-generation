{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r1gUGdHl_Km",
        "outputId": "2400cf09-75ea-4c98-bf1b-3eab20a52cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#load model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/diffusion/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "begxtyRdqyG-",
        "outputId": "b859c58f-3015-40bf-ede9-670332082dc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/diffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa==0.8.0\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_EJ8HqgKI3j",
        "outputId": "7047b1d2-49fe-4dae-ee56-2c5d4eef9d9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa==0.8.0 in /usr/local/lib/python3.8/dist-packages (0.8.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (1.0.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (0.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (1.21.6)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (1.2.0)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (0.11.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (0.56.4)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa==0.8.0) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa==0.8.0) (4.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa==0.8.0) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa==0.8.0) (0.39.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa==0.8.0) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa==0.8.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->pooch>=1.0->librosa==0.8.0) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa==0.8.0) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.8.0) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile>=0.9.0->librosa==0.8.0) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa==0.8.0) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa==0.8.0) (3.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.8/dist-packages (0.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzrnndt8v-bv",
        "outputId": "52667c14-305f-443a-cd10-dd87e582c838"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.8/dist-packages (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.8/dist-packages (from optuna) (4.1.0)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from optuna) (0.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.8.1)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.44)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<5.0.0->optuna) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (2.4.2)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (0.5.1)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna) (4.1.1)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from stevedore>=2.0.1->cliff->optuna) (5.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import librosa \n",
        "from glob import glob\n",
        "import tensorflow_hub as hub\n",
        "import soundfile as sf\n",
        "import optuna\n",
        "import random\n",
        "from functools import partial\n",
        "import warnings\n",
        "import IPython.display as ipd\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "UFYp2geyLaML"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ov-q_RMLq36",
        "outputId": "64c1e494-1d66-466b-8153-931870921867"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module = hub.KerasLayer('https://tfhub.dev/google/soundstream/mel/decoder/music/1')\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 1024\n",
        "HOP_LENGTH = 320\n",
        "WIN_LENGTH = 640\n",
        "N_MEL_CHANNELS = 128\n",
        "MEL_FMIN = 0.0\n",
        "MEL_FMAX = int(SAMPLE_RATE // 2)\n",
        "CLIP_VALUE_MIN = 1e-5\n",
        "CLIP_VALUE_MAX = 1e8\n",
        "\n",
        "MEL_BASIS = tf.signal.linear_to_mel_weight_matrix(\n",
        "    num_mel_bins=N_MEL_CHANNELS,\n",
        "    num_spectrogram_bins=N_FFT // 2 + 1,\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    lower_edge_hertz=MEL_FMIN,\n",
        "    upper_edge_hertz=MEL_FMAX)\n",
        "\n",
        "def calculate_spectrogram(samples):\n",
        "    \"\"\"Calculate mel spectrogram using the parameters the model expects.\"\"\"\n",
        "    fft = tf.signal.stft(\n",
        "      samples,\n",
        "      frame_length=WIN_LENGTH,\n",
        "      frame_step=HOP_LENGTH,\n",
        "      fft_length=N_FFT,\n",
        "      window_fn=tf.signal.hann_window,\n",
        "      pad_end=True)\n",
        "    fft_modulus = tf.abs(fft)\n",
        "\n",
        "    output = tf.matmul(fft_modulus, MEL_BASIS)\n",
        "\n",
        "    output = tf.clip_by_value(\n",
        "      output,\n",
        "      clip_value_min=CLIP_VALUE_MIN,\n",
        "      clip_value_max=CLIP_VALUE_MAX)\n",
        "    output = tf.math.log(output)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "0YcB0rsdLtg9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "music_files = glob(\"classical/*.wav\")\n",
        "data, sr = librosa.load(music_files[1], sr=SAMPLE_RATE)"
      ],
      "metadata": {
        "id": "1PVLTQqkMRfH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_samples = module([calculate_spectrogram(data)])"
      ],
      "metadata": {
        "id": "u51c2iaMMWZf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoidal_embedding(x):\n",
        "    embedding_min_frequency = 1.0\n",
        "    embedding_max_frequency = 1000.0\n",
        "    embedding_dims = 32\n",
        "    frequencies = tf.exp(\n",
        "        tf.linspace(\n",
        "            tf.math.log(embedding_min_frequency),\n",
        "            tf.math.log(embedding_max_frequency),\n",
        "            embedding_dims // 2,\n",
        "        )\n",
        "    )\n",
        "    angular_speeds = 2.0 * math.pi * frequencies\n",
        "    embeddings = tf.concat(\n",
        "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
        "    )\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def ResidualBlock(width):\n",
        "    def apply(x):\n",
        "        input_width = x.shape[3]\n",
        "        if input_width == width:\n",
        "            residual = x\n",
        "        else:\n",
        "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
        "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
        "        x = layers.Conv2D(\n",
        "            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
        "        )(x)\n",
        "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def DownBlock(width, block_depth):\n",
        "    def apply(x):\n",
        "        x, skips = x\n",
        "        for _ in range(block_depth):\n",
        "            x = ResidualBlock(width)(x)\n",
        "            skips.append(x)\n",
        "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def UpBlock(width, block_depth, attention=False):\n",
        "    def apply(x):\n",
        "        x, skips = x\n",
        "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
        "        for _ in range(block_depth):\n",
        "            skip = skips.pop()\n",
        "            x = layers.Concatenate()([x, skip] if not attention else [\n",
        "                x, skip, layers.MultiHeadAttention(\n",
        "                    num_heads=4, key_dim=1, attention_axes=(1, 2)\n",
        "                )(x, skip)\n",
        "            ])\n",
        "            x = ResidualBlock(width)(x)\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "def get_network(widths, block_depth, dim1=256, dim2=128, channels=1, attention=False):\n",
        "    noisy_input = keras.Input(shape=(dim1, dim2, channels))\n",
        "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
        "    \n",
        "    upsample_shape = (dim1, dim2)\n",
        "\n",
        "    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
        "    e = layers.UpSampling2D(size=upsample_shape, interpolation=\"nearest\")(e)\n",
        "\n",
        "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_input)\n",
        "    x = layers.Concatenate()([x, e])\n",
        "\n",
        "    skips = []\n",
        "    for width in widths[:-1]:\n",
        "        x = DownBlock(width, block_depth)([x, skips])\n",
        "\n",
        "    for _ in range(block_depth):\n",
        "        x = ResidualBlock(widths[-1])(x)\n",
        "\n",
        "    for idx, width in enumerate(reversed(widths[:-1])):\n",
        "        x = UpBlock(width, block_depth, attention=attention and idx == 0)([x, skips])\n",
        "\n",
        "    x = layers.Conv2D(channels, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
        "\n",
        "    return keras.Model([noisy_input, noise_variances], x, name=\"residual_unet\")\n",
        "\n",
        "\n",
        "class DDIM(keras.Model):\n",
        "    \"\"\"DDIM model modified from this tutorial: https://keras.io/examples/generative/ddim/\"\"\"\n",
        "    \n",
        "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128, min_signal_rate=0.02, max_signal_rate=0.95, ema = 0.999):\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalizer = layers.Normalization(axis=(2,3))\n",
        "        self.network = get_network(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2)\n",
        "        self.ema_network = keras.models.clone_model(self.network)\n",
        "        self.min_signal_rate = min_signal_rate\n",
        "        self.max_signal_rate = max_signal_rate\n",
        "        self.ema = ema\n",
        "\n",
        "    def compile(self, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "\n",
        "        self.noise_loss_tracker = keras.metrics.Mean(name=\"noise_loss\")\n",
        "        self.data_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.noise_loss_tracker, self.data_loss_tracker]\n",
        "\n",
        "    def denormalize(self, data):\n",
        "        data = self.normalizer.mean + data * self.normalizer.variance**0.5\n",
        "        return tf.clip_by_value(data, -128.0, 128.0)\n",
        "\n",
        "    def diffusion_schedule(self, diffusion_times):\n",
        "        start_angle = tf.acos(self.max_signal_rate)\n",
        "        end_angle = tf.acos(self.min_signal_rate)\n",
        "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
        "        signal_rates = tf.cos(diffusion_angles)\n",
        "        noise_rates = tf.sin(diffusion_angles)\n",
        "        return noise_rates, signal_rates\n",
        "\n",
        "    def denoise(self, noisy_data, noise_rates, signal_rates, training):\n",
        "        if training:\n",
        "            network = self.network\n",
        "        else:\n",
        "            network = self.ema_network\n",
        "        pred_noises = network([noisy_data, noise_rates**2], training=training)\n",
        "        pred_data = (noisy_data - noise_rates * pred_noises) / signal_rates\n",
        "\n",
        "        return pred_noises, pred_data\n",
        "\n",
        "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
        "        num_examples = tf.shape(initial_noise)[0]\n",
        "        step_size = 1.0 / diffusion_steps\n",
        "\n",
        "        # important line:\n",
        "        # at the first sampling step, the \"noisy data\" is pure noise\n",
        "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
        "        next_noisy_data = initial_noise\n",
        "        for step in tqdm(range(diffusion_steps)):\n",
        "            noisy_data = next_noisy_data\n",
        "\n",
        "            # separate the current noisy data to its components\n",
        "            diffusion_times = tf.ones((num_examples, 1, 1, 1)) - step * step_size\n",
        "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "            pred_noises, pred_data = self.denoise(\n",
        "                noisy_data, noise_rates, signal_rates, training=False\n",
        "            )\n",
        "            # network used in eval mode\n",
        "\n",
        "            # remix the predicted components using the next signal and noise rates\n",
        "            next_diffusion_times = diffusion_times - step_size\n",
        "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
        "                next_diffusion_times\n",
        "            )\n",
        "            next_noisy_data = (\n",
        "                next_signal_rates * pred_data + next_noise_rates * pred_noises\n",
        "            )\n",
        "            # this new noisy data will be used in the nexnt step\n",
        "\n",
        "        return pred_data\n",
        "\n",
        "    def generate(self, num_examples, shape, diffusion_steps):\n",
        "        # noise -> data -> denormalized data\n",
        "        initial_noise = tf.random.normal(shape=(num_examples, shape[0], shape[1], shape[2]))\n",
        "        generated_data = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
        "        generated_data = self.denormalize(generated_data)\n",
        "        return generated_data\n",
        "\n",
        "    def train_step(self, data):\n",
        "        batch_size = tf.shape(data)[0]\n",
        "        # normalize data to have standard deviation of 1, like the noises\n",
        "        data = self.normalizer(data, training=True)\n",
        "        noises = tf.random.normal(shape=tf.shape(data))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the data with noises accordingly\n",
        "        noisy_data = signal_rates * data + noise_rates * noises\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # train the network to separate noisy data to their components\n",
        "            pred_noises, pred_data = self.denoise(\n",
        "                noisy_data, noise_rates, signal_rates, training=True\n",
        "            )\n",
        "\n",
        "            noise_loss = self.loss(noises, pred_noises)  # used for training\n",
        "            data_loss = self.loss(data, pred_data)  # only used as metric\n",
        "\n",
        "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
        "\n",
        "        self.noise_loss_tracker.update_state(noise_loss)\n",
        "        self.data_loss_tracker.update_state(data_loss)\n",
        "\n",
        "        # track the exponential moving averages of weights\n",
        "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
        "            ema_weight.assign(self.ema * ema_weight + (1 - self.ema) * weight)\n",
        "\n",
        "        # KID is not measured during the training phase for computational efficiency\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # normalize data to have standard deviation of 1, like the noises\n",
        "        batch_size = tf.shape(data)[0]\n",
        "        \n",
        "        data = self.normalizer(data, training=False)\n",
        "        noises = tf.random.normal(shape=tf.shape(data))\n",
        "\n",
        "        # sample uniform random diffusion times\n",
        "        diffusion_times = tf.random.uniform(\n",
        "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
        "        )\n",
        "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
        "        # mix the data with noises accordingly\n",
        "        noisy_data = signal_rates * data + noise_rates * noises\n",
        "\n",
        "        # use the network to separate noisy data to their components\n",
        "        pred_noises, pred_data = self.denoise(\n",
        "            noisy_data, noise_rates, signal_rates, training=False\n",
        "        )\n",
        "\n",
        "        noise_loss = self.loss(noises, pred_noises)\n",
        "        data_loss = self.loss(data, pred_data)\n",
        "\n",
        "        self.data_loss_tracker.update_state(data_loss)\n",
        "        self.noise_loss_tracker.update_state(noise_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "UkUYEJxJq6aU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_at_interval(x, rate=10_000, feats=256, duration=3.3):\n",
        "    \"\"\"Load music from file at some offset. Return MDCT spectrogram of that data\"\"\"\n",
        "    file = x[0].numpy().decode()\n",
        "    idx = x[1].numpy()\n",
        "    audio, sr = librosa.load(file, duration=duration, sr=rate, offset=idx)\n",
        "    audio_fill = np.zeros(int(rate*duration), dtype=np.float32)\n",
        "    audio_fill[:len(audio)] = audio\n",
        "    spec = calculate_spectrogram(audio_fill)#spec = tf.signal.mdct(audio_fill, feats)\n",
        "    return spec\n",
        "\n",
        "def load_audio(x,y, rate=10_000, mdct_feats=256, duration=3.3):\n",
        "    \"\"\"TF function for loading MDCT spectrogram from file.\"\"\"\n",
        "    out = tf.py_function(lambda x,y: load_at_interval( \n",
        "        (x,y), rate=rate, feats=mdct_feats, duration=duration\n",
        "    ), inp=[x,y], Tout=tf.float32)\n",
        "    return out\n",
        "\n",
        "def get_files_dataset(\n",
        "        glob_location,\n",
        "        total_seconds=2,\n",
        "        out_len = 5.12,\n",
        "        hop_size=1,\n",
        "        max_feats = 2048,\n",
        "        batch_size=4,\n",
        "        shuffer_size=1000,\n",
        "        scale=1,\n",
        "        rate= 44100,#16_000 maestro, 44100\n",
        "        mdct_feats=256\n",
        "    ):\n",
        "    \"\"\"Get file dataset loader for a glob of audio files.\"\"\"\n",
        "    \n",
        "    files = glob(\n",
        "        glob_location,\n",
        "        recursive=True\n",
        "    )\n",
        "    \n",
        "    def file_list_generator():\n",
        "        for _ in range(total_seconds):\n",
        "            for file in files:\n",
        "                yield file, _*hop_size\n",
        "                \n",
        "    load_fn = partial(load_audio, duration=out_len, rate=rate, mdct_feats=mdct_feats)\n",
        "                \n",
        "    dg =tf.data.Dataset.from_generator(file_list_generator, output_signature = (\n",
        "        tf.TensorSpec(shape=(), dtype=tf.string), \n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32))).shuffle(shuffer_size).map(\n",
        "            load_fn, num_parallel_calls=tf.data.AUTOTUNE\n",
        "        ).map(\n",
        "            lambda x: tf.expand_dims(x, -1)[:max_feats, :, :]*scale\n",
        "        ).map(\n",
        "            lambda x: tf.ensure_shape(x, (max_feats, mdct_feats//2, 1))\n",
        "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dg"
      ],
      "metadata": {
        "id": "ixfthbz1q9TP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min_signal_rate = 0.02\n",
        "# max_signal_rate = 0.95\n",
        "\n",
        "# # architecture\n",
        "# embedding_dims = 32\n",
        "# embedding_max_frequency = 1000.0\n",
        "# widths = [32, 64, 96, 128]\n",
        "# block_depth = 2\n",
        "\n",
        "# # optimization\n",
        "# batch_size = 64\n",
        "# ema = 0.999\n",
        "# learning_rate = 1e-3\n",
        "# weight_decay = 1e-4\n",
        "\n",
        "# min_signal_rate = 0.02\n",
        "# max_signal_rate = 0.95\n",
        "# ema = 0.999"
      ],
      "metadata": {
        "id": "jboUgUL79jA1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gudgud96/frechet-audio-distance.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RViHrt83LpaL",
        "outputId": "b2490f22-0722-40f5-f60c-1508975a495b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'frechet-audio-distance' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install frechet_audio_distance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v63yqBtjS1hu",
        "outputId": "c04ae29c-04ff-4258-d054-383e48b0a100"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: frechet_audio_distance in /usr/local/lib/python3.8/dist-packages (0.0.4)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.8/dist-packages (from frechet_audio_distance) (0.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from frechet_audio_distance) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from frechet_audio_distance) (1.21.6)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (from frechet_audio_distance) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from frechet_audio_distance) (1.13.0+cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from frechet_audio_distance) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.8/dist-packages (from resampy->frechet_audio_distance) (0.56.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.53->resampy->frechet_audio_distance) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.53->resampy->frechet_audio_distance) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.53->resampy->frechet_audio_distance) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.53->resampy->frechet_audio_distance) (3.11.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile->frechet_audio_distance) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile->frechet_audio_distance) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->frechet_audio_distance) (4.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from frechet_audio_distance import FrechetAudioDistance"
      ],
      "metadata": {
        "id": "XJcRg718S8jB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionTuningObjective:  \n",
        "    def __init__(self,):\n",
        "      self.music_files = glob(\"classical/*.wav\")\n",
        "      self.data, self.sr = librosa.load(music_files[1], sr=SAMPLE_RATE)\n",
        "      \n",
        "\n",
        "    \n",
        "    def get_params(self, trial) -> dict:\n",
        "        return {\n",
        "        \"batch_size\": trial.suggest_int(\"batch_size\", 16, 256, 64),\n",
        "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 3e-5, 3e-1),\n",
        "        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-1),\n",
        "        \"ema\": trial.suggest_float(\"ema\", 0.95 , 0.99),\n",
        "        \"min_signal_rate\": trial.suggest_float(\"min_signal_rate\", 0.01, 0.20),\n",
        "        \"max_signal_rate\": trial.suggest_float(\"max_signal_rate\", 0.89, 0.98),\n",
        "        \"block_depth\": trial.suggest_int(\"block_depth\", 2, 4, 2)\n",
        "        }\n",
        "     \n",
        "    def __call__(self, trial):\n",
        "        params = self.get_params(trial)\n",
        "        dataset = get_files_dataset(\n",
        "            \"classical/*.wav\", \n",
        "            out_len=5.12, \n",
        "            max_feats=256, \n",
        "            total_seconds=20, \n",
        "            scale=1,\n",
        "            batch_size=params['batch_size']\n",
        "        )\n",
        "        for test_batch in dataset.take(1):\n",
        "          self.shape = test_batch.shape\n",
        "        num_total_examples = (len(self.music_files) * 25) // self.shape[0]\n",
        "        model = DDIM(widths = [32, 64, 96, 128], block_depth = params['block_depth'], attention=True, dim1=self.shape[1], dim2=self.shape[2],min_signal_rate= params['min_signal_rate'], max_signal_rate=params['max_signal_rate'], ema = params['ema'])\n",
        "        model.normalizer.adapt(dataset, steps=100)\n",
        "        model.compile(\n",
        "        loss=tf.keras.losses.MSE,\n",
        "        optimizer=tfa.optimizers.AdamW(\n",
        "            learning_rate = 3e-4,\n",
        "            weight_decay = 1e-4\n",
        "        ))\n",
        "        dataset = dataset.cache()\n",
        "        history = model.fit(dataset.repeat(), steps_per_epoch=num_total_examples, epochs=100)\n",
        "        specs = model.generate(32, self.shape[1:], 1000)\n",
        "\n",
        "        EXPERIMENT_NAME = \"test_save_model\"\n",
        "\n",
        "        model_results_path = f\"generated/{EXPERIMENT_NAME}\"\n",
        "        #create the director if it doesnt exist already\n",
        "        if os.path.exists(model_results_path) == False:\n",
        "          os.mkdir(model_results_path)\n",
        "\n",
        "        for i in range(len(specs)):\n",
        "          print(i)\n",
        "          sf.write(f'{model_results_path}/tone{i}.wav', module([specs[i, :, :, 0]]).numpy().T, SAMPLE_RATE)\n",
        "        frechet = FrechetAudioDistance(\n",
        "            use_pca=False, \n",
        "            use_activation=False,\n",
        "            verbose=False\n",
        "        )\n",
        "        fad_score = frechet.score(model_results_path, 'classical')\n",
        "        return fad_score"
      ],
      "metadata": {
        "id": "Sx177ofsXPGU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(DiffusionTuningObjective(),n_trials=10)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "print(time.time() - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQBqTjToTQxI",
        "outputId": "ca86fdbb-a1d5-457d-9686-22539ccd162f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-12-14 03:40:32,713]\u001b[0m A new study created in memory with name: no-name-d93604d3-eb2c-4b9f-86ba-85564f34f18f\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "156/156 [==============================] - 3996s 26s/step - noise_loss: 0.2640 - reconstruction_loss: 0.6641\n",
            "Epoch 2/100\n",
            "156/156 [==============================] - 3838s 25s/step - noise_loss: 0.1246 - reconstruction_loss: 0.1570\n",
            "Epoch 3/100\n",
            "156/156 [==============================] - 3848s 25s/step - noise_loss: 0.1116 - reconstruction_loss: 0.1385\n",
            "Epoch 4/100\n",
            " 96/156 [=================>............] - ETA: 24:56 - noise_loss: 0.1057 - reconstruction_loss: 0.1281"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "frkizlORTqVT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}